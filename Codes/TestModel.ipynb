{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import  f1_score, mean_squared_error, r2_score, make_scorer, accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost.callback import EarlyStopping\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_DAYS = 10\n",
    "PAST_DAYS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change these two to change models\n",
    "pm25 = False\n",
    "improvement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_prefix = '../Models/saved_models'\n",
    "folder_suffix = [\n",
    "    'yes_pm25' if pm25 else 'no_pm25',\n",
    "    'yes_improvement' if improvement else 'no_improvement'\n",
    "]\n",
    "models_folder = f\"{folder_prefix}_{'_'.join(folder_suffix)}\"\n",
    "\n",
    "models_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [f'AQI_cat_target_{day}' for day in range(N_DAYS)]\n",
    "\n",
    "models_dict = {'XGB': {}, 'LGBM': {}, 'CatBoost': {}}\n",
    "\n",
    "for model_name in ['XGB', 'LGBM', 'CatBoost']:\n",
    "    for target in targets:\n",
    "        fn = os.path.join(models_folder, f'{model_name}_{target}_model.pkl')\n",
    "        try:\n",
    "            models_dict[model_name][target] = joblib.load(fn)\n",
    "            print(f\"Loaded {fn}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Model file {fn} not found.\")\n",
    "\n",
    "best_weights_list = np.load(os.path.join(models_folder, 'best_weights.npy'), allow_pickle=True)\n",
    "thresholds_per_day = np.load(os.path.join(models_folder, 'thresholds_per_day.npy'), allow_pickle=True)\n",
    "print(\"Loaded best_weights_list and thresholds_per_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Inputs/data_onkk.csv')\n",
    "population_data = pd.read_excel('../Inputs/population_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, population_data, on='SID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total data records: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time'] = pd.to_datetime(data['time'], format='%m/%d/%Y')\n",
    "data['day'] = data['time'].dt.day\n",
    "data['month'] = data['time'].dt.month\n",
    "data['year'] = data['time'].dt.year\n",
    "\n",
    "data = data.sort_values(by=['SID', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "def feature_engineering(df):\n",
    "    if not improvement:\n",
    "        return df\n",
    "\n",
    "    df['season'] = df['month'].apply(get_season)\n",
    "    df['diffusion_conditions'] = df['WSPD'] * df['TP']\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = feature_engineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_features = ['WSPD', 'WDIR', 'TMP', 'TX', 'TN', 'TP', 'RH', 'PRES2M']\n",
    "\n",
    "if improvement:\n",
    "    lag_features += ['diffusion_conditions']\n",
    "\n",
    "if pm25:\n",
    "    lag_features += ['pm25']\n",
    "\n",
    "for lag in range(1, PAST_DAYS + 1):\n",
    "    data[f'time_lag_{lag}'] = data['time'] - pd.Timedelta(days=lag)\n",
    "    for feature in lag_features: \n",
    "        data = data.merge(\n",
    "            data[['SID', 'time', feature]].rename(columns={'time': f'time_lag_{lag}', feature: f'{feature}_lag_{lag}'}),\n",
    "            on=['SID', f'time_lag_{lag}'], how='left'\n",
    "        )\n",
    "\n",
    "    data = data.drop(columns=[f'time_lag_{lag}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in range(0, N_DAYS):\n",
    "    data[f'time_target_{day}'] = data['time'] + pd.Timedelta(days=day)\n",
    "    data = data.merge(\n",
    "        data[['SID', 'time', 'pm25']].rename(columns={'time': f'time_target_{day}', 'pm25': f'pm25_target_{day}'}),\n",
    "        on=['SID', f'time_target_{day}'], how='left'\n",
    "    )\n",
    "    data = data.drop(columns=[f'time_target_{day}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[f'pm25_target_{day}' for day in range(0, N_DAYS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total data records: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pm25_to_aqi(pm25):\n",
    "    bp = [\n",
    "        (0, 25, 0, 50),\n",
    "        (25, 50, 51, 100),\n",
    "        (50, 80, 101, 150),\n",
    "        (80, 150, 151, 200),\n",
    "        (150, 250, 201, 300),\n",
    "        (250, 350, 301, 400),\n",
    "        (350, 500, 401, 500),\n",
    "        (500, float('inf'), 501, 500) \n",
    "    ]\n",
    "    \n",
    "    for (bp_low, bp_high, i_low, i_high) in bp:\n",
    "        if bp_low <= pm25 < bp_high:\n",
    "            aqi = ((i_high - i_low) / (bp_high - bp_low)) * (pm25 - bp_low) + i_low\n",
    "            return min(round(aqi), i_high) \n",
    "    return 500\n",
    "\n",
    "def aqi_category(aqi):\n",
    "    if aqi <= 50:\n",
    "        return 0 \n",
    "    elif 51 <= aqi <= 100:\n",
    "        return 1 \n",
    "    elif 101 <= aqi <= 150:\n",
    "        return 2 \n",
    "    elif 151 <= aqi <= 200:\n",
    "        return 3 \n",
    "    elif 201 <= aqi <= 300:\n",
    "        return 4 \n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "vectorized_pm25_to_aqi = np.vectorize(pm25_to_aqi)\n",
    "vectorized_aqi_category = np.vectorize(aqi_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in range(0, N_DAYS):\n",
    "    feature = f'pm25_target_{day}'\n",
    "    target = f'AQI_cat_target_{day}'\n",
    "    data[target] = vectorized_pm25_to_aqi(data[feature])\n",
    "    data[target] = vectorized_aqi_category(data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ([\n",
    "    'SQRT_SEA_DEM_LAT'] +\n",
    "    [f'{feature}_lag_{i}' for feature in lag_features for i in range(1, PAST_DAYS + 1)]\n",
    ")\n",
    "\n",
    "if improvement:\n",
    "    features += ['urbanization_rate', 'population_density', 'season']\n",
    "\n",
    "targets = [f'AQI_cat_target_{day}' for day in range(0, N_DAYS)]\n",
    "\n",
    "X = data[features]\n",
    "y = data[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time = (data['time'] >= '2021-06-01') & (data['time'] <= '2021-12-31')\n",
    "\n",
    "X_test = X.loc[test_time]\n",
    "y_test = y.loc[test_time]\n",
    "\n",
    "X_train_full = X.loc[~test_time]\n",
    "y_train_full = y.loc[~test_time]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    test_size=0.25,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "total_samples = X.shape[0]\n",
    "\n",
    "splits = {\n",
    "    \"Train\": X_train,\n",
    "    \"Validation\": X_valid,\n",
    "    \"Test\": X_test\n",
    "}\n",
    "\n",
    "for name, X_split in splits.items():\n",
    "    n = X_split.shape[0]\n",
    "    pct = n / total_samples * 100\n",
    "    print(f\"{name} samples: {n} ({pct:.2f}% of total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X_train.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "    X_test[col] = label_encoder.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_valid = pd.DataFrame(X_valid, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds_test = np.column_stack([\n",
    "    models_dict['XGB'][target].predict(xgb.DMatrix(X_test))\n",
    "    for target in targets\n",
    "])\n",
    "\n",
    "lgbm_preds_test = np.column_stack([\n",
    "    models_dict['LGBM'][target].predict(X_test)\n",
    "    for target in targets\n",
    "])\n",
    "\n",
    "catboost_preds_test = np.column_stack([\n",
    "    models_dict['CatBoost'][target].predict(X_test)\n",
    "    for target in targets\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_with_thresholds(aqi_vals, th):\n",
    "    return np.digitize(aqi_vals, np.sort(th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = y_test.values.copy()\n",
    "test_accuracy_per_day = []\n",
    "test_precision_per_day = []\n",
    "test_recall_per_day = []\n",
    "\n",
    "ncols = 3\n",
    "nrows = (len(targets) + ncols - 1) // ncols\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for day in range(len(targets)):\n",
    "    w_xgb, w_lgbm, w_cat = best_weights_list[day]\n",
    "    final_preds_test = (\n",
    "        w_xgb  * xgb_preds_test[:, day] +\n",
    "        w_lgbm * lgbm_preds_test[:, day] +\n",
    "        w_cat  * catboost_preds_test[:, day]\n",
    "    )\n",
    "    rounded_preds_test = round_with_thresholds(final_preds_test, thresholds_per_day[day])\n",
    "    \n",
    "    acc = accuracy_score(y_test_np[:, day], rounded_preds_test)\n",
    "    prec = precision_score(y_test_np[:, day], rounded_preds_test, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test_np[:, day], rounded_preds_test, average='weighted', zero_division=0)\n",
    "    print(f\"Day {day}: Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}\")\n",
    "    \n",
    "    test_accuracy_per_day.append(acc)\n",
    "    test_precision_per_day.append(prec)\n",
    "    test_recall_per_day.append(rec)\n",
    "    \n",
    "    classes = np.unique(np.concatenate([y_test_np[:, day], rounded_preds_test]))\n",
    "    cm = confusion_matrix(y_test_np[:, day], rounded_preds_test, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    \n",
    "    ax = axes[day]\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=True, xticks_rotation='vertical')\n",
    "    \n",
    "    ax.set_title(f'Confusion Matrix\\nDay {day}')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "\n",
    "for i in range(len(targets), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(models_folder + '/confusion_matrices.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6986612,
     "sourceId": 11191616,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7032567,
     "sourceId": 11253522,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7032579,
     "sourceId": 11253540,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
